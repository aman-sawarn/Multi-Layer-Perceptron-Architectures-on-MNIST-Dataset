{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfj_hrrDPl9Q"
   },
   "source": [
    "## MLP: Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7618,
     "status": "ok",
     "timestamp": 1524233979321,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "fIymnakkW_ef",
    "outputId": "a8906b86-0e2d-4d3d-823e-e5afab4c3ae1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-56dc335eeb48>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "## MLP: Initializations# https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/multi_layer_perceptron_mnist.html\n",
    "# https://github.com/wagonhelm/NaNmnist/blob/master/NaNmnist.ipynb\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/neural_network.ipynb\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kCIcM92XW_ek"
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4\n",
    "# https://stackoverflow.com/a/14434334\n",
    "# this function is used to update the plots for each epoch and error\n",
    "def plt_dynamic(x, y, y_1, ax, ticks,title, colors=['b']):\n",
    "    ax.plot(x, y, 'b', label=\"Train Loss\")\n",
    "    ax.plot(x, y_1, 'r', label=\"Test Loss\")\n",
    "    if len(x)==1:\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "    plt.yticks(ticks)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2ado_38xW_em"
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 512 # 1st layer number of neurons\n",
    "n_hidden_2 = 128 # 2nd layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QxBA530YW_eo"
   },
   "outputs": [],
   "source": [
    "# x isn't a specific value. It's a placeholder, a value that we'll input when we ask TensorFlow to run a computation.\n",
    "# We want to be able to input any number of MNIST images, each flattened into a 784-dimensional vector. \n",
    "# We represent this as a 2-D tensor of floating-point numbers, with a shape X = [None, 784]. \n",
    "# (Here None means that a dimension can be of any length.)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# keep_prob: we will be using these placeholders when we use dropouts, while testing model\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# keep_prob_input: we will be using these placeholders when we use dropouts, while training model\n",
    "keep_prob_input = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "y7UGi3bTW_eq"
   },
   "outputs": [],
   "source": [
    "# Weight initialization\n",
    "\n",
    "# https://arxiv.org/pdf/1707.09725.pdf#page=95\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random_normal\n",
    "# Outputs random values from a normal distribution mean=0 std=1\n",
    "\n",
    "# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni+ni+1). \n",
    "# h1 =>  σ=√(2/(fan_in+fan_out+1) = 0.039  => N(0,σ) = N(0,0.039)\n",
    "# h2 =>  σ=√(2/(fan_in+fan_out+1) = 0.055  => N(0,σ) = N(0,0.055)\n",
    "# out =>  σ=√(2/(fan_in+fan_out+1) = 0.120  => N(0,σ) = N(0,0.120)\n",
    "# SGD: Xavier/Glorot Normal initialization.\n",
    "weights_sgd = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.039, mean=0)),    #784x512 # sqrt(2/(784+512)) = 0.039\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.055, mean=0)), #512x128 # sqrt(2/(512+128)) = 0.055\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n",
    "}\n",
    "\n",
    "# https://arxiv.org/pdf/1707.09725.pdf#page=95\n",
    "# for relu lates\n",
    "# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni). \n",
    "# h1 =>  σ=√(2/(fan_in+1) = 0.062  => N(0,σ) = N(0,0.062)\n",
    "# h2 =>  σ=√(2/(fan_in+1) = 0.125  => N(0,σ) = N(0,0.125)\n",
    "# out =>  σ=√(2/(fan_in+1) = 0.120  => N(0,σ) = N(0,0.120)\n",
    "# He Normal initialization.\n",
    "weights_relu = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.062, mean=0)),    #784x512\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.125, mean=0)), #512x128\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #512x1\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #128x1\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "coLzoyq6W_es"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D17GOa7EW_eu"
   },
   "source": [
    "<h3> Model 1: input (784) - sigmoid(512) - sigmoid(128) - \n",
    "softmax(output 10) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "785WfoDgW_ev"
   },
   "outputs": [],
   "source": [
    "# https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/multi_layer_perceptron_mnist.html\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with Sigmoid activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with Sigmoid activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    # Output layer with Sigmoid activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFbSVmCLW_ex"
   },
   "source": [
    "__ Model 1 + AdamOptimizer __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 105057,
     "status": "ok",
     "timestamp": 1524234093572,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "e_lbs-NrW_ex",
    "outputId": "13beb44a-0b07-4816-a04e-de0f3fdb55e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (?, 784) W[h1]: (784, 512) b[h1]: (512,)\n",
      "layer_1: (?, 512) W[h2]: (512, 128) b[h2]: (128,)\n",
      "layer_2: (?, 128) W[out]: (128, 10) b3: (10,)\n",
      "out_layer: (?, 10)\n",
      "WARNING:tensorflow:From <ipython-input-8-d2b008b566d8>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since we are using sigmoid activations in hiden layers we will be using weights that are initalized as weights_sgd\n",
    "y_sgd = multilayer_perceptron(x, weights_sgd, biases)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "cost_sgd = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_sgd, labels = y_))\n",
    "\n",
    "# https://github.com/amitmac/Question-Answering/issues/2\n",
    "# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \n",
    "optimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
    "optimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here c: corresponds to the parameter cost_sgd\n",
    "            # w : correspondse to the parameter weights_sgd\n",
    "            # c = sess.run() return the cost after every bath during train\n",
    "            # w = sess.run() return the weights that are modified after every batch through Back prop\n",
    "            # w is dict w = {'h1': updated h1 weight vector after the current batch,\n",
    "            #                'h2': updated h2 weight vector after the current batch, \n",
    "            #                'out': updated output weight vector after the current batch, \n",
    "            #                }\n",
    "            # you check these w matrix for every iteration, and check whats happening during back prop\n",
    "            #\n",
    "            # note: sess.run() returns parameter values based on the input parameters\n",
    "            # _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd]) it returns three parameters\n",
    "            # _, c = sess.run([optimizer_adam, cost_sgd ]) it returns two parameters\n",
    "            # _ = sess.run([optimizer_adam]) it returns one paramter (for the input optimizer it return none)\n",
    "            # c = sess.run([cost_sgd]) it returns one paramter (for the input cost return error after the batch)\n",
    "\n",
    "            # feed_dict={x: batch_xs, y_: batch_ys} here x, y_ should be placeholders\n",
    "            # x, y_ are the input parameters on which the models gets trained\n",
    "\n",
    "            # here we use AdamOptimizer\n",
    "            _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_sgd, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3764,
     "status": "ok",
     "timestamp": 1524234097387,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "XwNhAhptW_e0",
    "outputId": "625444b0-9b9b-46ae-8502-f0e58332c55e"
   },
   "outputs": [],
   "source": [
    "# Plot weight distriubtion at the end of training.\n",
    "\n",
    "import seaborn as sns\n",
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czgq-TZoW_e3"
   },
   "source": [
    "__ Model 1 + GradientDescentOptimizer __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 102906,
     "status": "ok",
     "timestamp": 1524234200348,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "rb7IfZdAW_e4",
    "outputId": "bc198d80-97d9-4393-8a00-d3bd7ad21e4c"
   },
   "outputs": [],
   "source": [
    "# We can now launch the model in an InteractiveSession\n",
    "\n",
    "# We first have to create an operation to initialize the variables we created:\n",
    "# https://github.com/amitmac/Question-Answering/issues/2\n",
    "\n",
    "# Note: make sure you initialize variables.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use GradientDescentOptimizer\n",
    "            _, c, w = sess.run([optimizer_sgdc, cost_sgd, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_sgd, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3576,
     "status": "ok",
     "timestamp": 1524234204031,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "jdEv1u4XW_e7",
    "outputId": "917d096b-2626-42f9-c265-24ee3a5fc518"
   },
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DjEzlRxW_e-"
   },
   "source": [
    "<h3> Model 2: input (784) - ReLu(512) - ReLu(128) - sigmoid(output 10) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LhfDXfE1W_e_"
   },
   "outputs": [],
   "source": [
    "# https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/multi_layer_perceptron_mnist.html\n",
    "# Create model\n",
    "def multilayer_perceptron_relu(x, weights, biases):\n",
    "    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with ReLu activation\n",
    "    # https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with ReLu activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    # Output layer with Sigmoid activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6JHDIYCW_fC"
   },
   "source": [
    "__ Input-ReLu(512)-ReLu(128)-sigmoid(output) - AdamOptimizer  __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 104642,
     "status": "ok",
     "timestamp": 1524234309984,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "0QG071qMW_fC",
    "outputId": "834c2a6a-8f74-45c6-f8ec-d62f9a3ac9d6"
   },
   "outputs": [],
   "source": [
    "# Since we are using Relu activations in hiden layers we will be using weights that are initalized as weights_relu\n",
    "yrelu = multilayer_perceptron_relu(x, weights_relu, biases)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "cost_relu = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = yrelu, labels = y_))\n",
    "# https://github.com/amitmac/Question-Answering/issues/2\n",
    "# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \n",
    "optimizer_relu_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_relu)\n",
    "optimizer_relu_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_relu)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use AdamOptimizer\n",
    "            _, c, w = sess.run([optimizer_relu_adam, cost_relu, weights_relu], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_relu, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "    # plot final results\n",
    "    plt_dynamic(xs, ytrs, ytes, ax,np.arange(1.3, 1.8, step=0.04), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(yrelu,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3570,
     "status": "ok",
     "timestamp": 1524234313622,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "bzI38kakW_fG",
    "outputId": "ceb9d536-d278-46b6-8359-c2af890d816c"
   },
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "33lgT4C-W_fK"
   },
   "source": [
    "__Input-ReLu(512)-ReLu(128)-sigmoid(output) - GradientDescentOptimizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 102428,
     "status": "ok",
     "timestamp": 1524234416089,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "kRgWQJVeW_fK",
    "outputId": "733618aa-6d6d-4cd8-f03a-4eb92ba8c0a3"
   },
   "outputs": [],
   "source": [
    "# We can now launch the model in an InteractiveSession\n",
    "\n",
    "# We first have to create an operation to initialize the variables we created:\n",
    "# https://github.com/amitmac/Question-Answering/issues/2\n",
    "\n",
    "# Note: make sure you initialize variables after AdamOptimizer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use GradientDescentOptimizer\n",
    "            _, c, w = sess.run([optimizer_relu_sgdc, cost_relu, weights_relu], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_relu, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "    # plot final results\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(yrelu,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3649,
     "status": "ok",
     "timestamp": 1524234419782,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "qGRKjIhHW_fN",
    "outputId": "22aab1a9-ff45-442f-8b16-81e39d16e351"
   },
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gGc2FJXW_fQ"
   },
   "source": [
    "<h3> Model 3: Input - Sigmoid(BatchNormalization(512)) - Sigmoid(BatchNormalization(128))- Sigmoid(output) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5dlFEuJbW_fR"
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization\n",
    "# https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "epsilon = 1e-3\n",
    "def multilayer_perceptron_batch(x, weights, biases):\n",
    "    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    \n",
    "    ############################################################\n",
    "    # Hidden layer with Sigmoid activation and batch normalization\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "    # Calculate the mean and variance of x.\n",
    "    batch_mean_1, batch_var_1 = tf.nn.moments(layer_1,[0])\n",
    "    \n",
    "    scale_1 = tf.Variable(tf.ones([n_hidden_1]))\n",
    "    beta_1 = tf.Variable(tf.zeros([n_hidden_1]))\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization\n",
    "    layer_1 = tf.nn.batch_normalization(layer_1, batch_mean_1, batch_var_1, beta_1, scale_1, epsilon)\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    \n",
    "    #####################################################################################\n",
    "    \n",
    "    # Hidden layer with Sigmoid activation and batch normalization\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/moments\n",
    "    # Calculate the mean and variance of x.\n",
    "    batch_mean_2, batch_var_2 = tf.nn.moments(layer_2, [0])\n",
    "    \n",
    "    scale_2 = tf.Variable(tf.ones([n_hidden_2]))\n",
    "    beta_2 = tf.Variable(tf.zeros([n_hidden_2]))\n",
    "    \n",
    "    layer_2 = tf.nn.batch_normalization(layer_2, batch_mean_2, batch_var_2, beta_2, scale_2, epsilon)\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    ######################################################################################\n",
    "    \n",
    "    # output layer with Sigmoid activation \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2wqpTvpW_fS"
   },
   "source": [
    "__ Model 3+ AdamOptimizer  __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 144410,
     "status": "ok",
     "timestamp": 1524234565528,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "oy91z8l5W_fU",
    "outputId": "07173615-05d2-4a9e-dc53-e01d4293153a"
   },
   "outputs": [],
   "source": [
    "# Since we are using sigmoid activations in hiden layers we will be using weights that are initalized as weights_sgd\n",
    "ybatch = multilayer_perceptron_batch(x, weights_sgd, biases)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "cost_batch = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ybatch, labels = y_))\n",
    "# https://github.com/amitmac/Question-Answering/issues/2\n",
    "# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \n",
    "optimizer_batch_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_batch)\n",
    "optimizer_batch_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_batch)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use AdamOptimizer\n",
    "            _, c, w = sess.run([optimizer_batch_adam, cost_batch, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_batch, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "    # plot final results\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(ybatch,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3504,
     "status": "ok",
     "timestamp": 1524234569062,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "O3xoNRl6W_fW",
    "outputId": "918cc851-d9b4-4674-f011-a6749bc89a5a"
   },
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfGy1atZW_fY"
   },
   "source": [
    "__ Model 3 + GradientDescentOptimizer  __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 140869,
     "status": "ok",
     "timestamp": 1524234709951,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "rNT58FJ_W_fZ",
    "outputId": "a8f05d58-5cc7-48b0-b66e-3845bc070a52"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # here we use GradientDescentOptimizer\n",
    "            _, c, w = sess.run([optimizer_batch_sgdc, cost_batch, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_batch, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax,np.arange(1.5, 2.4, step=0.05), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "    # plot final results\n",
    "    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-GradientDescentOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(ybatch,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3688,
     "status": "ok",
     "timestamp": 1524234713701,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "6DD7yotPW_fc",
    "outputId": "683ebefe-1ebe-4641-97ce-e3bfc458dd11"
   },
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHON-IgYW_ff"
   },
   "source": [
    "<h3> Model 4: Input - ReLu(512) - Dropout - ReLu(128)- Dropout -Sigmoid(output) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JiYI-p4yW_ff"
   },
   "outputs": [],
   "source": [
    "# https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/multi_layer_perceptron_mnist.html\n",
    "# Create model\n",
    "def multilayer_perceptron_dropout(x, weights, biases):\n",
    "    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n",
    "    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n",
    "    # we are adding a drop out layer after input layers with parameter keep_prob_input\n",
    "    \n",
    "    # Hidden layer with ReLu activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # we are adding a drop out layer after the first hidden layer with parameter keep_prob\n",
    "    layer_1_drop = tf.nn.dropout(layer_1, keep_prob)\n",
    "    \n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n",
    "    \n",
    "    # Hidden layer with ReLu activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1_drop, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # we are adding a drop out layer after the first hidden layer with parameter keep_prob\n",
    "    layer_2_drop = tf.nn.dropout(layer_2, keep_prob)\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    \n",
    "    # Output layer with Sigmoid activation\n",
    "    out_layer = tf.matmul(layer_2_drop, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcWWmqY3W_fi"
   },
   "source": [
    "__ Model 4 + AdamOptimizer __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 125115,
     "status": "ok",
     "timestamp": 1524234840061,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "i0XcrYQxW_fi",
    "outputId": "b5864676-2557-4f3b-8142-981516475c5f"
   },
   "outputs": [],
   "source": [
    "# Since we are using Relu activations in hiden layers we will be using weights that are initalized as weights_relu\n",
    "ydrop = multilayer_perceptron_dropout(x, weights_relu, biases)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "cost_drop = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ydrop, labels = y_))\n",
    "# https://github.com/amitmac/Question-Answering/issues/2\n",
    "# there are many optimizers available: https://www.tensorflow.org/versions/r1.2/api_guides/python/train#Optimizers \n",
    "optimizer_drop_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_drop)\n",
    "optimizer_drop_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_drop)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n",
    "    xs, ytrs, ytes = [], [], []\n",
    "    for epoch in range(training_epochs):\n",
    "        train_avg_cost = 0.\n",
    "        test_avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # here we use AdamOptimizer\n",
    "            _, c, w = sess.run([optimizer_drop_adam, cost_drop, weights_relu], feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "            train_avg_cost += c / total_batch\n",
    "            c = sess.run(cost_drop, feed_dict={x: mnist.test.images, y_: mnist.test.labels,  keep_prob: 1.0})\n",
    "            test_avg_cost += c / total_batch\n",
    "\n",
    "        xs.append(epoch)\n",
    "        ytrs.append(train_avg_cost)\n",
    "        ytes.append(test_avg_cost)\n",
    "        plt_dynamic(xs, ytrs, ytes, ax,np.arange(1, 1.8, step=0.05), \"input-ReLu(512)-Dropout-ReLu(128)-Dropout-Sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "        if epoch%display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n",
    "\n",
    "    # plot final results\n",
    "    plt_dynamic(xs, ytrs, ytes, ax,np.arange(1, 1.8, step=0.05), \"input-ReLu(512)-Dropout-ReLu(128)-Dropout-Sigmoid(output)-AdamOptimizer\")\n",
    "\n",
    "    # we are calculating the final accuracy on the test data\n",
    "    correct_prediction = tf.equal(tf.argmax(ydrop,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0 }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3338,
     "status": "ok",
     "timestamp": 1524234844950,
     "user": {
      "displayName": "Brahma Reddy",
      "photoUrl": "//lh3.googleusercontent.com/-yCpFTUxFtZQ/AAAAAAAAAAI/AAAAAAAAa8w/qMMF8qrlVGo/s50-c-k-no/photo.jpg",
      "userId": "106404213545378824111"
     },
     "user_tz": -330
    },
    "id": "7sp3S3JlW_fo",
    "outputId": "bb98cf34-dd34-460a-a479-0d14613b2ddd"
   },
   "outputs": [],
   "source": [
    "h1_w = w['h1'].flatten().reshape(-1,1)\n",
    "h2_w = w['h2'].flatten().reshape(-1,1)\n",
    "out_w = w['out'].flatten().reshape(-1,1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Weight matrix\")\n",
    "ax = sns.violinplot(y=h1_w,color='b')\n",
    "plt.xlabel('Hidden Layer 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=h2_w, color='r')\n",
    "plt.xlabel('Hidden Layer 2 ')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Weight matrix \")\n",
    "ax = sns.violinplot(y=out_w,color='y')\n",
    "plt.xlabel('Output Layer ')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "MLP_Tensorflow_drive.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
