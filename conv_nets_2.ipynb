{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv_nets_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sawarn69/Multi-Layer-Perceptron-Architectures-on-MNIST-Dataset/blob/master/conv_nets_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrEx8xE-XgxF",
        "colab_type": "code",
        "outputId": "c74e12a7-ac99-498e-a046-d282014242ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
        "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
        "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(56, kernel_size=(3,3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Dropout(0.45))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           verbose=1,\n",
        "#           validation_data=(x_test, y_test))\n",
        "# score = model.evaluate(x_test, y_test, verbose=0)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 290s 5ms/step - loss: 0.2693 - acc: 0.9160 - val_loss: 0.0493 - val_acc: 0.9838\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 291s 5ms/step - loss: 0.0861 - acc: 0.9742 - val_loss: 0.0368 - val_acc: 0.9874\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 299s 5ms/step - loss: 0.0649 - acc: 0.9810 - val_loss: 0.0313 - val_acc: 0.9887\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0527 - acc: 0.9842 - val_loss: 0.0257 - val_acc: 0.9908\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0464 - acc: 0.9863 - val_loss: 0.0236 - val_acc: 0.9926\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0409 - acc: 0.9880 - val_loss: 0.0219 - val_acc: 0.9927\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0385 - acc: 0.9890 - val_loss: 0.0223 - val_acc: 0.9920\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0374 - acc: 0.9891 - val_loss: 0.0204 - val_acc: 0.9930\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0343 - acc: 0.9897 - val_loss: 0.0185 - val_acc: 0.9940\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0315 - acc: 0.9906 - val_loss: 0.0202 - val_acc: 0.9931\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0295 - acc: 0.9909 - val_loss: 0.0186 - val_acc: 0.9944\n",
            "Epoch 12/20\n",
            "56960/60000 [===========================>..] - ETA: 14s - loss: 0.0281 - acc: 0.9912"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P41dJcHYDraj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGbfQwIeDrhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "fig,ax = plt.subplots(1,1)\n",
        "ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
        "\n",
        "# list of epoch numbers\n",
        "x = list(range(1,21))\n",
        "\n",
        "# print(history.history.keys())\n",
        "# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
        "# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
        "\n",
        "# we will get val_loss and val_acc only when you pass the paramter validation_data\n",
        "# val_loss : validation loss\n",
        "# val_acc : validation accuracy\n",
        "\n",
        "# loss : training loss\n",
        "# acc : train accuracy\n",
        "# for each key in histrory.histrory we will have a list of length equal to number of epochs\n",
        "\n",
        "vy = history.history['val_loss']\n",
        "ty = history.history['loss']\n",
        "plt_dynamic(x, vy, ty, ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "633pf8JhiYdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(80, kernel_size=(3,3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Dropout(0.45))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(48, (7, 7), activation='relu'))\n",
        "model.add(Dropout(0.30))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           verbose=1,\n",
        "#           validation_data=(x_test, y_test))\n",
        "# score = model.evaluate(x_test, y_test, verbose=0)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezvra-AHiv2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "fig,ax = plt.subplots(1,1)\n",
        "ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
        "\n",
        "# list of epoch numbers\n",
        "x = list(range(1,21))\n",
        "\n",
        "# print(history.history.keys())\n",
        "# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
        "# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
        "\n",
        "# we will get val_loss and val_acc only when you pass the paramter validation_data\n",
        "# val_loss : validation loss\n",
        "# val_acc : validation accuracy\n",
        "\n",
        "# loss : training loss\n",
        "# acc : train accuracy\n",
        "# for each key in histrory.histrory we will have a list of length equal to number of epochs\n",
        "\n",
        "vy = history.history['val_loss']\n",
        "ty = history.history['loss']\n",
        "plt_dynamic(x, vy, ty, ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRJMS_2vEBTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}