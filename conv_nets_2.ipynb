{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv_nets_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sawarn69/Multi-Layer-Perceptron-Architectures-on-MNIST-Dataset/blob/master/conv_nets_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrEx8xE-XgxF",
        "colab_type": "code",
        "outputId": "c74e12a7-ac99-498e-a046-d282014242ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1066
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
        "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
        "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    fig.canvas.draw()\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(56, kernel_size=(3,3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Dropout(0.45))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           verbose=1,\n",
        "#           validation_data=(x_test, y_test))\n",
        "# score = model.evaluate(x_test, y_test, verbose=0)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 290s 5ms/step - loss: 0.2693 - acc: 0.9160 - val_loss: 0.0493 - val_acc: 0.9838\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 291s 5ms/step - loss: 0.0861 - acc: 0.9742 - val_loss: 0.0368 - val_acc: 0.9874\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 299s 5ms/step - loss: 0.0649 - acc: 0.9810 - val_loss: 0.0313 - val_acc: 0.9887\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0527 - acc: 0.9842 - val_loss: 0.0257 - val_acc: 0.9908\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0464 - acc: 0.9863 - val_loss: 0.0236 - val_acc: 0.9926\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0409 - acc: 0.9880 - val_loss: 0.0219 - val_acc: 0.9927\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0385 - acc: 0.9890 - val_loss: 0.0223 - val_acc: 0.9920\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0374 - acc: 0.9891 - val_loss: 0.0204 - val_acc: 0.9930\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0343 - acc: 0.9897 - val_loss: 0.0185 - val_acc: 0.9940\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0315 - acc: 0.9906 - val_loss: 0.0202 - val_acc: 0.9931\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0295 - acc: 0.9909 - val_loss: 0.0186 - val_acc: 0.9944\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 299s 5ms/step - loss: 0.0278 - acc: 0.9913 - val_loss: 0.0175 - val_acc: 0.9946\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0265 - acc: 0.9915 - val_loss: 0.0181 - val_acc: 0.9941\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 299s 5ms/step - loss: 0.0237 - acc: 0.9926 - val_loss: 0.0177 - val_acc: 0.9943\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 300s 5ms/step - loss: 0.0237 - acc: 0.9923 - val_loss: 0.0175 - val_acc: 0.9950\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0225 - acc: 0.9930 - val_loss: 0.0179 - val_acc: 0.9953\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 297s 5ms/step - loss: 0.0208 - acc: 0.9935 - val_loss: 0.0192 - val_acc: 0.9940\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 297s 5ms/step - loss: 0.0211 - acc: 0.9933 - val_loss: 0.0229 - val_acc: 0.9926\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 297s 5ms/step - loss: 0.0191 - acc: 0.9939 - val_loss: 0.0174 - val_acc: 0.9948\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0198 - acc: 0.9939 - val_loss: 0.0161 - val_acc: 0.9948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P41dJcHYDraj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGbfQwIeDrhI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "f15d5a71-ddc0-453d-cddb-ed2016f08822"
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "fig,ax = plt.subplots(1,1)\n",
        "ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
        "\n",
        "# list of epoch numbers\n",
        "x = list(range(1,21))\n",
        "\n",
        "# print(history.history.keys())\n",
        "# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
        "# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
        "\n",
        "# we will get val_loss and val_acc only when you pass the paramter validation_data\n",
        "# val_loss : validation loss\n",
        "# val_acc : validation accuracy\n",
        "\n",
        "# loss : training loss\n",
        "# acc : train accuracy\n",
        "# for each key in histrory.histrory we will have a list of length equal to number of epochs\n",
        "\n",
        "vy = history.history['val_loss']\n",
        "ty = history.history['loss']\n",
        "plt_dynamic(x, vy, ty, ax)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.016059770254798104\n",
            "Test accuracy: 0.9948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsmRhjDEmIksWxhhjIvr/W2+AqMoGeNkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "633pf8JhiYdx",
        "colab_type": "code",
        "outputId": "c99563f2-1d34-4ca7-8e39-eddce3344166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(80, kernel_size=(3,3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Dropout(0.45))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(48, (7, 7), activation='relu'))\n",
        "model.add(Dropout(0.30))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model.fit(x_train, y_train,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           verbose=1,\n",
        "#           validation_data=(x_test, y_test))\n",
        "# score = model.evaluate(x_test, y_test, verbose=0)\n",
        "# print('Test loss:', score[0])\n",
        "# print('Test accuracy:', score[1])\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=20, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 604s 10ms/step - loss: 11.3449 - acc: 0.2423 - val_loss: 11.4410 - val_acc: 0.2891\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 597s 10ms/step - loss: 10.6260 - acc: 0.3404 - val_loss: 10.1139 - val_acc: 0.3719\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 597s 10ms/step - loss: 10.3209 - acc: 0.3594 - val_loss: 8.3320 - val_acc: 0.4826\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 600s 10ms/step - loss: 10.4085 - acc: 0.3540 - val_loss: 7.7088 - val_acc: 0.5208\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 601s 10ms/step - loss: 9.1658 - acc: 0.4311 - val_loss: 6.7155 - val_acc: 0.5826\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 599s 10ms/step - loss: 8.7204 - acc: 0.4588 - val_loss: 7.1725 - val_acc: 0.5545\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 599s 10ms/step - loss: 8.4791 - acc: 0.4738 - val_loss: 6.8665 - val_acc: 0.5736\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 599s 10ms/step - loss: 8.8181 - acc: 0.4527 - val_loss: 7.3844 - val_acc: 0.5416\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 596s 10ms/step - loss: 9.0196 - acc: 0.4403 - val_loss: 8.4777 - val_acc: 0.4737\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 598s 10ms/step - loss: 9.1748 - acc: 0.4307 - val_loss: 7.6523 - val_acc: 0.5250\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 597s 10ms/step - loss: 8.6392 - acc: 0.4639 - val_loss: 7.0313 - val_acc: 0.5636\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 600s 10ms/step - loss: 8.5426 - acc: 0.4699 - val_loss: 6.7243 - val_acc: 0.5826\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 601s 10ms/step - loss: 8.1410 - acc: 0.4949 - val_loss: 5.8491 - val_acc: 0.6369\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 605s 10ms/step - loss: 8.0536 - acc: 0.5003 - val_loss: 7.3999 - val_acc: 0.5408\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 599s 10ms/step - loss: 8.0756 - acc: 0.4989 - val_loss: 7.0812 - val_acc: 0.5606\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 600s 10ms/step - loss: 8.0883 - acc: 0.4981 - val_loss: 6.9102 - val_acc: 0.5712\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 598s 10ms/step - loss: 8.1108 - acc: 0.4968 - val_loss: 7.0710 - val_acc: 0.5613\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 601s 10ms/step - loss: 7.9636 - acc: 0.5059 - val_loss: 6.2292 - val_acc: 0.6135\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 600s 10ms/step - loss: 7.5191 - acc: 0.5335 - val_loss: 5.6455 - val_acc: 0.6496\n",
            "Epoch 20/20\n",
            "29184/60000 [=============>................] - ETA: 4:57 - loss: 7.3110 - acc: 0.5463"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezvra-AHiv2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "fig,ax = plt.subplots(1,1)\n",
        "ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n",
        "\n",
        "# list of epoch numbers\n",
        "x = list(range(1,21))\n",
        "\n",
        "# print(history.history.keys())\n",
        "# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
        "# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
        "\n",
        "# we will get val_loss and val_acc only when you pass the paramter validation_data\n",
        "# val_loss : validation loss\n",
        "# val_acc : validation accuracy\n",
        "\n",
        "# loss : training loss\n",
        "# acc : train accuracy\n",
        "# for each key in histrory.histrory we will have a list of length equal to number of epochs\n",
        "\n",
        "vy = history.history['val_loss']\n",
        "ty = history.history['loss']\n",
        "plt_dynamic(x, vy, ty, ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRJMS_2vEBTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}